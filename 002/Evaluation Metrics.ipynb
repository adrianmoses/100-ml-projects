{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bfa8d23",
   "metadata": {},
   "source": [
    "# Evaluation Metrics of Supervised Problems\n",
    "\n",
    "## Classification metrics\n",
    "\n",
    "- Accuracy\n",
    "- Precision (P)\n",
    "- Recall (R)\n",
    "- Area under the ROC (Receiver Operating Characteristic) AUC\n",
    "- Log loss\n",
    "- Precision at k (P@k)\n",
    "- Averation precision at k (AP@k)\n",
    "- Mean average precision at k (MAP@k)\n",
    "\n",
    "## Regresion metrics\n",
    "\n",
    "- Mean absolute error (MAE)\n",
    "- Mean squared error (MSE)\n",
    "- Root mean squared error (RMSE)\n",
    "- Root mean squared logarithmic error (RMSLE)\n",
    "- Mean percentage error (MPE)\n",
    "- Mean absolute percentage error (MAPE)\n",
    "- R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50f208",
   "metadata": {},
   "source": [
    "### Example: Binary Classification Problem\n",
    "\n",
    "Let's learn about metrics with  a sample problem. Classifiying chest x-ray images. \n",
    "\n",
    "Some images are of collapsed lung known as pneumothorax. This is what we want to classify\n",
    "\n",
    "100 positive x-ray images\n",
    "100 negative images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c954418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a balanced sample 50/50 images\n",
    "# a balanced set makes it easy to use the metrics\n",
    "# accuracy, precision, recall, and F1\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: accuracy score\n",
    "    \"\"\"\n",
    "    # initialize a simple counter for correct predictions\n",
    "    correct_counter = 0\n",
    "    # loop over all elements of y_true\n",
    "    # and y_pred \"together\"\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == yp:\n",
    "            # if prediction is equal to truh, increase the counter\n",
    "            correct_counter += 1\n",
    "    \n",
    "    # return accuracy\n",
    "    # which is correct predictions over the number of samples\n",
    "    return correct_counter / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6338bee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "l1 = [0, 1, 1, 1, 0, 0, 0, 1]\n",
    "l2 = [0, 1, 0, 1, 0, 1, 0, 0]\n",
    "\n",
    "metrics.accuracy_score(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49e295",
   "metadata": {},
   "source": [
    "### Unbalanced datasets\n",
    "\n",
    "For unbalanced datasets accuracy is a misleading metric because of data skew\n",
    "\n",
    "It's better to look at precision or the ratite of positive and negative classes\n",
    "\n",
    "True Positive (TP) - a positve value that was predicted as positive\n",
    "\n",
    "True Negative (TN) - a negative value that was predicted as negative\n",
    "\n",
    "False Positive (FP) - a negative value that was predicted as positive\n",
    "\n",
    "False Negative (FN) - a negative value that was predicted as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e0c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
