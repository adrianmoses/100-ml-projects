{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "For text classification there's\n",
    "\n",
    "- binary classification (and specifically sentiment analyis)\n",
    "- multi-class classification\n",
    "- and for scoring or ranking docs, there's regression (Resserria)\n",
    "\n",
    "\n",
    "sententence pair tasks such as semantic similairity and textual entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "model_path = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path, \n",
    "    id2label={0: 'NEG', 1: 'POS'}, \n",
    "    label2id={\"NEG\": 0, \"POS\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4000, 2), (1000, 2), (1000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_train= load_dataset('imdb', split=\"train[:2000]+train[-2000:]\")\n",
    "imdb_test= load_dataset('imdb', split=\"test[:500]+test[-500:]\")\n",
    "imdb_val= load_dataset('imdb', split=\"test[500:1000]+test[-1000:-500]\")\n",
    "imdb_train.shape, imdb_test.shape, imdb_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-ee1693c1a4681e24.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8c552cf5854f6e83881bcd90a8678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a/cache-6c7ea317d8e73ef2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now tokenize for training\n",
    "tknize = lambda e: tokenizer(e['text'], padding=True, truncation=True)\n",
    "enc_train = imdb_train.map(tknize, batched=True, batch_size=1000)\n",
    "enc_test = imdb_test.map(tknize, batched=True, batch_size=1000)\n",
    "enc_val = imdb_val.map(tknize, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 22953, 2213, 4381, 2152, 2003, 1037, 947...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 11573, 2791, 1006, 2030, 2160, 24913, 20...</td>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 8235, 2058, 1011, 3772, 2011, 23920, 575...</td>\n",
       "      <td>1</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2023, 2003, 4089, 1996, 2087, 2104, 9250...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2023, 2003, 2025, 1996, 5171, 11463, 837...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2875, 1996, 2203, 1997, 1996, 3185, 1010...</td>\n",
       "      <td>0</td>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2023, 2003, 1996, 2785, 1997, 3185, 2008...</td>\n",
       "      <td>0</td>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 1045, 2387, 1005, 6934, 1005, 2197, 2305...</td>\n",
       "      <td>0</td>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2070, 3152, 2008, 2017, 4060, 2039, 2005...</td>\n",
       "      <td>0</td>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[101, 2023, 2003, 2028, 1997, 1996, 12873, 435...</td>\n",
       "      <td>0</td>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         attention_mask  \\\n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "3995  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3996  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3997  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3998  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3999  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              input_ids  label  \\\n",
       "0     [101, 22953, 2213, 4381, 2152, 2003, 1037, 947...      1   \n",
       "1     [101, 11573, 2791, 1006, 2030, 2160, 24913, 20...      1   \n",
       "2     [101, 8235, 2058, 1011, 3772, 2011, 23920, 575...      1   \n",
       "3     [101, 2023, 2003, 4089, 1996, 2087, 2104, 9250...      1   \n",
       "4     [101, 2023, 2003, 2025, 1996, 5171, 11463, 837...      1   \n",
       "...                                                 ...    ...   \n",
       "3995  [101, 2875, 1996, 2203, 1997, 1996, 3185, 1010...      0   \n",
       "3996  [101, 2023, 2003, 1996, 2785, 1997, 3185, 2008...      0   \n",
       "3997  [101, 1045, 2387, 1005, 6934, 1005, 2197, 2305...      0   \n",
       "3998  [101, 2070, 3152, 2008, 2017, 4060, 2039, 2005...      0   \n",
       "3999  [101, 2023, 2003, 2028, 1997, 1996, 12873, 435...      0   \n",
       "\n",
       "                                                   text  \n",
       "0     Bromwell High is a cartoon comedy. It ran at t...  \n",
       "1     Homelessness (or Houselessness as George Carli...  \n",
       "2     Brilliant over-acting by Lesley Ann Warren. Be...  \n",
       "3     This is easily the most underrated film inn th...  \n",
       "4     This is not the typical Mel Brooks film. It wa...  \n",
       "...                                                 ...  \n",
       "3995  Towards the end of the movie, I felt it was to...  \n",
       "3996  This is the kind of movie that my enemies cont...  \n",
       "3997  I saw 'Descent' last night at the Stockholm Fi...  \n",
       "3998  Some films that you pick up for a pound turn o...  \n",
       "3999  This is one of the dumbest films, I've ever se...  \n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(enc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./MyIMDBModel',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    fp16=cuda.is_available(),\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Text classifications are usually evalutated with micro-averaging or macro-averaging F1\n",
    "\n",
    "macro-averaging gives euql weight to each class\n",
    "micro-averaging is # times model is correct / total # of decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=enc_train,\n",
    "    eval_dataset=enc_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madrianmoses\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/adrianmoses/huggingface/runs/2zfhotrx\" target=\"_blank\">./MyIMDBModel</a></strong> to <a href=\"https://wandb.ai/adrianmoses/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 16:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.681800</td>\n",
       "      <td>0.601343</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.835994</td>\n",
       "      <td>0.836048</td>\n",
       "      <td>0.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.412913</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.801270</td>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.293118</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.886006</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.315500</td>\n",
       "      <td>0.321727</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.888438</td>\n",
       "      <td>0.897005</td>\n",
       "      <td>0.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.311679</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.874293</td>\n",
       "      <td>0.883632</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.270497</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.909987</td>\n",
       "      <td>0.910236</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.282820</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.894681</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.302467</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.908989</td>\n",
       "      <td>0.909198</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.300530</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.908985</td>\n",
       "      <td>0.909277</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.312986</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.915988</td>\n",
       "      <td>0.916240</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.330631</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>0.908934</td>\n",
       "      <td>0.910196</td>\n",
       "      <td>0.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.344561</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.914981</td>\n",
       "      <td>0.915374</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.393872</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.907928</td>\n",
       "      <td>0.909284</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.366316</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.909956</td>\n",
       "      <td>0.910795</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.356754</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.913991</td>\n",
       "      <td>0.914166</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-50\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-50/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-100\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-100/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-150\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-150/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-150/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-200\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-200/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-200/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-250\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-250/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-250/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-300\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-300/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-300/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-350\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-350/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-350/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-400\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-400/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-400/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-450\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-450/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-450/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-500\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-500/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-550\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-550/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-550/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-600\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-600/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-600/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-650\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-650/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-650/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-700\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-700/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-700/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./MyIMDBModel/checkpoint-750\n",
      "Configuration saved in ./MyIMDBModel/checkpoint-750/config.json\n",
      "Model weights saved in ./MyIMDBModel/checkpoint-750/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./MyIMDBModel/checkpoint-300 (score: 0.27049708366394043).\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_Accuracy</th>\n",
       "      <th>eval_F1</th>\n",
       "      <th>eval_Precision</th>\n",
       "      <th>eval_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.138274</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>0.953499</td>\n",
       "      <td>0.953537</td>\n",
       "      <td>0.9535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.270497</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.909987</td>\n",
       "      <td>0.910236</td>\n",
       "      <td>0.9100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.296340</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>0.897993</td>\n",
       "      <td>0.898102</td>\n",
       "      <td>0.8980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eval_loss  eval_Accuracy   eval_F1  eval_Precision  eval_Recall\n",
       "train   0.138274         0.9535  0.953499        0.953537       0.9535\n",
       "val     0.270497         0.9100  0.909987        0.910236       0.9100\n",
       "test    0.296340         0.8980  0.897993        0.898102       0.8980"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [trainer.evaluate(eval_dataset=data) for data in [enc_train, enc_val, enc_test]]\n",
    "pd.DataFrame(q, index=[\"train\", \"val\", \"test\"]).iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def get_prediction(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True,\n",
    "                      max_length=250, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(inputs[\"input_ids\"].to(device), inputs[\"attention_mask\"].to(device))\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs, probs.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I didn't like the move it bored me\"\n",
    "get_prediction(text)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to MyBestIMDBModel\n",
      "Configuration saved in MyBestIMDBModel/config.json\n",
      "Model weights saved in MyBestIMDBModel/pytorch_model.bin\n",
      "tokenizer config file saved in MyBestIMDBModel/tokenizer_config.json\n",
      "Special tokens file saved in MyBestIMDBModel/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('MyBestIMDBModel/tokenizer_config.json',\n",
       " 'MyBestIMDBModel/special_tokens_map.json',\n",
       " 'MyBestIMDBModel/vocab.txt',\n",
       " 'MyBestIMDBModel/added_tokens.json',\n",
       " 'MyBestIMDBModel/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"MyBestIMDBModel\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file MyBestIMDBModel/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"POS\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file MyBestIMDBModel/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at MyBestIMDBModel.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "Didn't find file MyBestIMDBModel/added_tokens.json. We won't load it.\n",
      "loading file MyBestIMDBModel/vocab.txt\n",
      "loading file MyBestIMDBModel/tokenizer.json\n",
      "loading file None\n",
      "loading file MyBestIMDBModel/special_tokens_map.json\n",
      "loading file MyBestIMDBModel/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"MyBestIMDBModel\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"MyBestIMDBModel\")\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9832347631454468}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"the move was very impressive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEG', 'score': 0.9751225709915161}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"the text of the picture was very poor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
