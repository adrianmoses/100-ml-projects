{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: transformers in /usr/local/lib/python3.6/dist-packages (4.11.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tokenizers in /usr/local/lib/python3.6/dist-packages (0.10.3)\r\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\r\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2021.10.8)\r\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\r\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\r\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.51.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\r\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.0.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.19)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (2.0.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.25.0)\r\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->transformers) (1.15.0)\r\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\r\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\r\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.10.0.2)\r\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.26.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\r\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.6)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9764a606104b689aff6cc5ed476d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bcbef908cf42a5ab4839319773a50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=385.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a0d412786944c2a65d30f5746a59b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=262620.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC size is: 32000\n",
      "The model is: <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizerTUR = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-uncased\")\n",
    "print(f\"VOC size is: {tokenizerTUR.vocab_size}\")\n",
    "print(f\"The model is: {type(tokenizerTUR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31640f3d63d5428baebbdcec149e980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dfebd76dd74355bf0152e42a2a7936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3370c3c3bb5d4999a8c0c1b53f6e18a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10e7264774144b29c28ee0f08eba58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=466062.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC size is: 30522\n",
      "The model is: <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizerEN = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"VOC size is: {tokenizerEN.vocab_size}\")\n",
    "print(f\"The model is: {type(tokenizerEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is in Turkish Model ? False\n",
      "is in English Model ? True\n"
     ]
    }
   ],
   "source": [
    "word_en = \"telecommunication\"\n",
    "print(f\"is in Turkish Model ? {word_en in tokenizerTUR.vocab}\")\n",
    "print(f\"is in English Model ? {word_en in tokenizerEN.vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tel', '##eco', '##mm', '##un', '##ica', '##tion']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizerTUR.tokenize(word_en)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Unknown words are broken down into parts. The parts are tokens known to the model (in the vocab)\n",
    "\n",
    "The English model does know about the word, so it does not need to break it into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['telecommunication']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerEN.tokenize(word_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muvaffak',\n",
       " '##iyet',\n",
       " '##siz',\n",
       " '##les',\n",
       " '##tir',\n",
       " '##ici',\n",
       " '##les',\n",
       " '##tir',\n",
       " '##iver',\n",
       " '##emeye',\n",
       " '##bilecekleri',\n",
       " '##mi',\n",
       " '##z',\n",
       " '##den',\n",
       " '##mis',\n",
       " '##siniz',\n",
       " '##cesine']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_word_tur = \"Muvaffakiyetsizleştiricileştiriveremeyebileceklerimizdenmişsinizcesine\"\n",
    "tokenizerTUR.tokenize(long_word_tur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "This is the WordPiece Algorithmn. BERT, DistilBERT, and ELECTRA require a WordPiece tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.6.5)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/dist-packages (from nltk) (2021.10.8)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.51.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README\t\t       burgess-busterbrown.txt\tmilton-paradise.txt\r\n",
      "austen-emma.txt        carroll-alice.txt\tshakespeare-caesar.txt\r\n",
      "austen-persuasion.txt  chesterton-ball.txt\tshakespeare-hamlet.txt\r\n",
      "austen-sense.txt       chesterton-brown.txt\tshakespeare-macbeth.txt\r\n",
      "bible-kjv.txt\t       chesterton-thursday.txt\twhitman-leaves.txt\r\n",
      "blake-poems.txt        edgeworth-parents.txt\r\n",
      "bryant-stories.txt     melville-moby_dick.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/nltk_data/corpora/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plays = ['shakespeare-macbeth.txt', 'shakespeare-hamlet.txt', 'shakespeare-caesar.txt']\n",
    "shakespeare = [\" \".join(s) for ply in plays for s in gutenberg.sents(ply)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "temp_proc = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import (Sequence, Lowercase, NFD, StripAccents)\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import BPEDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Pre-processing:\n",
    "1. Normalizer\n",
    "2. Pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = BPEDecoder()\n",
    "tokenizer.post_processor = temp_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=5000,\n",
    "    special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train_from_iterator(shakespeare, trainer)\n",
    "print(f\"Trained vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'is', 'this', 'a', 'dagger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'hand', 'le', 'toward', 'my', 'hand', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sen = \"Is this a dagger which I see before me, the handle toward my hand?\"\n",
    "sen_enc = tokenizer.encode(sen)\n",
    "print(f\"Output: {format(sen_enc.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'macbeth', 'and', 'hu', 'gg', 'ing', 'face', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sen_enc2 = tokenizer.encode(\"Macbeth and Hugging Face\")\n",
    "print(f\"Output: {format(sen_enc2.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'i', 'like', 'hu', 'gg', 'in', 'f', 'face', '!', '[SEP]', 'he', 'likes', 'macbeth', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "two_enc = tokenizer.encode(\"I like Hugginf Face!\", \"He likes Macbeth!\")\n",
    "print(f\"Output: {format(two_enc.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.json', './merges.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4948 ./merges.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#version: 0.2 - Trained by `huggingface/tokenizers`\r\n",
      "t h\r\n",
      "o u\r\n",
      "a n\r\n",
      "th e\r\n",
      "r e\r\n"
     ]
    }
   ],
   "source": [
    "!head -6 ./merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ['[CLS]', 'i', 'still', 'like', 'hu', 'gg', 'ing', 'face', 'and', 'macbeth', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# save entire pipeline\n",
    "tokenizer.save(\"MyBPETokenizer.json\")\n",
    "tokenizerFromFile = Tokenizer.from_file(\"MyBPETokenizer.json\")\n",
    "sen_enc3 = tokenizerFromFile.encode(\"I still like Hugging Face and Macbeth\")\n",
    "print(f\"Output: {format(sen_enc3.tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "from tokenizers.normalizers import BertNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordPiece())\n",
    "tokenizer.normalizer = BertNormalizer()\n",
    "tokenizer.pre_torkinzer = Whitespace()\n",
    "tokenizer.decoder = WordPieceDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tokenizer object:\n",
      "\n",
      "class Tokenizer(builtins.object)\n",
      " |  A :obj:`Tokenizer` works as a pipeline. It processes some raw text as input\n",
      " |  and outputs an :class:`~tokenizers.Encoding`.\n",
      " |  \n",
      " |  Args:\n",
      " |      model (:class:`~tokenizers.models.Model`):\n",
      " |          The core algorithm that this :obj:`Tokenizer` should be using.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __getstate__(...)\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |  \n",
      " |  add_special_tokens(self, tokens)\n",
      " |      Add the given special tokens to the Tokenizer.\n",
      " |      \n",
      " |      If these tokens are already part of the vocabulary, it just let the Tokenizer know about\n",
      " |      them. If they don't exist, the Tokenizer creates them, giving them a new id.\n",
      " |      \n",
      " |      These special tokens will never be processed by the model (ie won't be split into\n",
      " |      multiple tokens), and they can be removed from the output when decoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
      " |              The list of special tokens we want to add to the vocabulary. Each token can either\n",
      " |              be a string or an instance of :class:`~tokenizers.AddedToken` for more\n",
      " |              customization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: The number of tokens that were created in the vocabulary\n",
      " |  \n",
      " |  add_tokens(self, tokens)\n",
      " |      Add the given tokens to the vocabulary\n",
      " |      \n",
      " |      The given tokens are added only if they don't already exist in the vocabulary.\n",
      " |      Each token then gets a new attributed id.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
      " |              The list of tokens we want to add to the vocabulary. Each token can be either a\n",
      " |              string or an instance of :class:`~tokenizers.AddedToken` for more customization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: The number of tokens that were created in the vocabulary\n",
      " |  \n",
      " |  decode(self, ids, skip_special_tokens=True)\n",
      " |      Decode the given list of ids back to a string\n",
      " |      \n",
      " |      This is used to decode anything coming back from a Language Model\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (A :obj:`List/Tuple` of :obj:`int`):\n",
      " |              The list of ids that we want to decode\n",
      " |      \n",
      " |          skip_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      " |              Whether the special tokens should be removed from the decoded string\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded string\n",
      " |  \n",
      " |  decode_batch(self, sequences, skip_special_tokens=True)\n",
      " |      Decode a batch of ids back to their corresponding string\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`List` of :obj:`List[int]`):\n",
      " |              The batch of sequences we want to decode\n",
      " |      \n",
      " |          skip_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      " |              Whether the special tokens should be removed from the decoded strings\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: A list of decoded strings\n",
      " |  \n",
      " |  enable_padding(self, direction='right', pad_id=0, pad_type_id=0, pad_token='[PAD]', length=None, pad_to_multiple_of=None)\n",
      " |      Enable the padding\n",
      " |      \n",
      " |      Args:\n",
      " |          direction (:obj:`str`, `optional`, defaults to :obj:`right`):\n",
      " |              The direction in which to pad. Can be either ``right`` or ``left``\n",
      " |      \n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If specified, the padding length should always snap to the next multiple of the\n",
      " |              given value. For example if we were going to pad witha length of 250 but\n",
      " |              ``pad_to_multiple_of=8`` then we will pad to 256.\n",
      " |      \n",
      " |          pad_id (:obj:`int`, defaults to 0):\n",
      " |              The id to be used when padding\n",
      " |      \n",
      " |          pad_type_id (:obj:`int`, defaults to 0):\n",
      " |              The type id to be used when padding\n",
      " |      \n",
      " |          pad_token (:obj:`str`, defaults to :obj:`[PAD]`):\n",
      " |              The pad token to be used when padding\n",
      " |      \n",
      " |          length (:obj:`int`, `optional`):\n",
      " |              If specified, the length at which to pad. If not specified we pad using the size of\n",
      " |              the longest sequence in a batch.\n",
      " |  \n",
      " |  enable_truncation(self, max_length, stride=0, strategy='longest_first')\n",
      " |      Enable truncation\n",
      " |      \n",
      " |      Args:\n",
      " |          max_length (:obj:`int`):\n",
      " |              The max length at which to truncate\n",
      " |      \n",
      " |          stride (:obj:`int`, `optional`):\n",
      " |              The length of the previous first sequence to be included in the overflowing\n",
      " |              sequence\n",
      " |      \n",
      " |          strategy (:obj:`str`, `optional`, defaults to :obj:`longest_first`):\n",
      " |              The strategy used to truncation. Can be one of ``longest_first``, ``only_first`` or\n",
      " |              ``only_second``.\n",
      " |  \n",
      " |  encode(self, sequence, pair=None, is_pretokenized=False, add_special_tokens=True)\n",
      " |      Encode the given sequence and pair. This method can process raw text sequences\n",
      " |      as well as already pre-tokenized sequences.\n",
      " |      \n",
      " |      Example:\n",
      " |          Here are some examples of the inputs that are accepted::\n",
      " |      \n",
      " |              encode(\"A single sequence\")`\n",
      " |              encode(\"A sequence\", \"And its pair\")`\n",
      " |              encode([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], is_pretokenized=True)`\n",
      " |              encode(\n",
      " |                  [ \"A\", \"pre\", \"tokenized\", \"sequence\" ], [ \"And\", \"its\", \"pair\" ],\n",
      " |                  is_pretokenized=True\n",
      " |              )\n",
      " |      \n",
      " |      Args:\n",
      " |          sequence (:obj:`~tokenizers.InputSequence`):\n",
      " |              The main input sequence we want to encode. This sequence can be either raw\n",
      " |              text or pre-tokenized, according to the ``is_pretokenized`` argument:\n",
      " |      \n",
      " |              - If ``is_pretokenized=False``: :class:`~tokenizers.TextInputSequence`\n",
      " |              - If ``is_pretokenized=True``: :class:`~tokenizers.PreTokenizedInputSequence`\n",
      " |      \n",
      " |          pair (:obj:`~tokenizers.InputSequence`, `optional`):\n",
      " |              An optional input sequence. The expected format is the same that for ``sequence``.\n",
      " |      \n",
      " |          is_pretokenized (:obj:`bool`, defaults to :obj:`False`):\n",
      " |              Whether the input is already pre-tokenized\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      " |              Whether to add the special tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~tokenizers.Encoding`: The encoded result\n",
      " |  \n",
      " |  encode_batch(self, input, is_pretokenized=False, add_special_tokens=True)\n",
      " |      Encode the given batch of inputs. This method accept both raw text sequences\n",
      " |      as well as already pre-tokenized sequences.\n",
      " |      \n",
      " |      Example:\n",
      " |          Here are some examples of the inputs that are accepted::\n",
      " |      \n",
      " |              encode_batch([\n",
      " |                  \"A single sequence\",\n",
      " |                  (\"A tuple with a sequence\", \"And its pair\"),\n",
      " |                  [ \"A\", \"pre\", \"tokenized\", \"sequence\" ],\n",
      " |                  ([ \"A\", \"pre\", \"tokenized\", \"sequence\" ], \"And its pair\")\n",
      " |              ])\n",
      " |      \n",
      " |      Args:\n",
      " |          input (A :obj:`List`/:obj:`Tuple` of :obj:`~tokenizers.EncodeInput`):\n",
      " |              A list of single sequences or pair sequences to encode. Each sequence\n",
      " |              can be either raw text or pre-tokenized, according to the ``is_pretokenized``\n",
      " |              argument:\n",
      " |      \n",
      " |              - If ``is_pretokenized=False``: :class:`~tokenizers.TextEncodeInput`\n",
      " |              - If ``is_pretokenized=True``: :class:`~tokenizers.PreTokenizedEncodeInput`\n",
      " |      \n",
      " |          is_pretokenized (:obj:`bool`, defaults to :obj:`False`):\n",
      " |              Whether the input is already pre-tokenized\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      " |              Whether to add the special tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          A :obj:`List` of :class:`~tokenizers.Encoding`: The encoded batch\n",
      " |  \n",
      " |  get_vocab(self, with_added_tokens=True)\n",
      " |      Get the underlying vocabulary\n",
      " |      \n",
      " |      Args:\n",
      " |          with_added_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      " |              Whether to include the added tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary\n",
      " |  \n",
      " |  get_vocab_size(self, with_added_tokens=True)\n",
      " |      Get the size of the underlying vocabulary\n",
      " |      \n",
      " |      Args:\n",
      " |          with_added_tokens (:obj:`bool`, defaults to :obj:`True`):\n",
      " |              Whether to include the added tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: The size of the vocabulary\n",
      " |  \n",
      " |  id_to_token(self, id)\n",
      " |      Convert the given id to its corresponding token if it exists\n",
      " |      \n",
      " |      Args:\n",
      " |          id (:obj:`int`):\n",
      " |              The id to convert\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Optional[str]`: An optional token, :obj:`None` if out of vocabulary\n",
      " |  \n",
      " |  no_padding(self)\n",
      " |      Disable padding\n",
      " |  \n",
      " |  no_truncation(self)\n",
      " |      Disable truncation\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, is_pair)\n",
      " |      Return the number of special tokens that would be added for single/pair sentences.\n",
      " |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n",
      " |      :return:\n",
      " |  \n",
      " |  post_process(self, encoding, pair=None, add_special_tokens=True)\n",
      " |      Apply all the post-processing steps to the given encodings.\n",
      " |      \n",
      " |      The various steps are:\n",
      " |      \n",
      " |          1. Truncate according to the set truncation params (provided with\n",
      " |             :meth:`~tokenizers.Tokenizer.enable_truncation`)\n",
      " |          2. Apply the :class:`~tokenizers.processors.PostProcessor`\n",
      " |          3. Pad according to the set padding params (provided with\n",
      " |             :meth:`~tokenizers.Tokenizer.enable_padding`)\n",
      " |      \n",
      " |      Args:\n",
      " |          encoding (:class:`~tokenizers.Encoding`):\n",
      " |              The :class:`~tokenizers.Encoding` corresponding to the main sequence.\n",
      " |      \n",
      " |          pair (:class:`~tokenizers.Encoding`, `optional`):\n",
      " |              An optional :class:`~tokenizers.Encoding` corresponding to the pair sequence.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`):\n",
      " |              Whether to add the special tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~tokenizers.Encoding`: The final post-processed encoding\n",
      " |  \n",
      " |  save(self, pretty=False)\n",
      " |      Save the :class:`~tokenizers.Tokenizer` to the file at the given path.\n",
      " |      \n",
      " |      Args:\n",
      " |          path (:obj:`str`):\n",
      " |              A path to a file in which to save the serialized tokenizer.\n",
      " |      \n",
      " |          pretty (:obj:`bool`, defaults to :obj:`False`):\n",
      " |              Whether the JSON file should be pretty formatted.\n",
      " |  \n",
      " |  to_str(self, pretty=False)\n",
      " |      Gets a serialized string representing this :class:`~tokenizers.Tokenizer`.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretty (:obj:`bool`, defaults to :obj:`False`):\n",
      " |              Whether the JSON string should be pretty formatted.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: A string representing the serialized Tokenizer\n",
      " |  \n",
      " |  token_to_id(self, token)\n",
      " |      Convert the given token to its corresponding id if it exists\n",
      " |      \n",
      " |      Args:\n",
      " |          token (:obj:`str`):\n",
      " |              The token to convert\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Optional[int]`: An optional id, :obj:`None` if out of vocabulary\n",
      " |  \n",
      " |  train(self, files, trainer=None)\n",
      " |      Train the Tokenizer using the given files.\n",
      " |      \n",
      " |      Reads the files line by line, while keeping all the whitespace, even new lines.\n",
      " |      If you want to train from data store in-memory, you can check\n",
      " |      :meth:`~tokenizers.Tokenizer.train_from_iterator`\n",
      " |      \n",
      " |      Args:\n",
      " |          files (:obj:`List[str]`):\n",
      " |              A list of path to the files that we should use for training\n",
      " |      \n",
      " |          trainer (:obj:`~tokenizers.trainers.Trainer`, `optional`):\n",
      " |              An optional trainer that should be used to train our Model\n",
      " |  \n",
      " |  train_from_iterator(self, iterator, trainer=None, length=None)\n",
      " |      Train the Tokenizer using the provided iterator.\n",
      " |      \n",
      " |      You can provide anything that is a Python Iterator\n",
      " |      \n",
      " |          * A list of sequences :obj:`List[str]`\n",
      " |          * A generator that yields :obj:`str` or :obj:`List[str]`\n",
      " |          * A Numpy array of strings\n",
      " |          * ...\n",
      " |      \n",
      " |      Args:\n",
      " |          iterator (:obj:`Iterator`):\n",
      " |              Any iterator over strings or list of strings\n",
      " |      \n",
      " |          trainer (:obj:`~tokenizers.trainers.Trainer`, `optional`):\n",
      " |              An optional trainer that should be used to train our Model\n",
      " |      \n",
      " |          length (:obj:`int`, `optional`):\n",
      " |              The total number of sequences in the iterator. This is used to\n",
      " |              provide meaningful progress tracking\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_buffer(buffer)\n",
      " |      Instantiate a new :class:`~tokenizers.Tokenizer` from the given buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |          buffer (:obj:`bytes`):\n",
      " |              A buffer containing a previously serialized :class:`~tokenizers.Tokenizer`\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~tokenizers.Tokenizer`: The new tokenizer\n",
      " |  \n",
      " |  from_file(path)\n",
      " |      Instantiate a new :class:`~tokenizers.Tokenizer` from the file at the given path.\n",
      " |      \n",
      " |      Args:\n",
      " |          path (:obj:`str`):\n",
      " |              A path to a local JSON file representing a previously serialized\n",
      " |              :class:`~tokenizers.Tokenizer`\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~tokenizers.Tokenizer`: The new tokenizer\n",
      " |  \n",
      " |  from_str(json)\n",
      " |      Instantiate a new :class:`~tokenizers.Tokenizer` from the given JSON string.\n",
      " |      \n",
      " |      Args:\n",
      " |          json (:obj:`str`):\n",
      " |              A valid JSON string representing a previously serialized\n",
      " |              :class:`~tokenizers.Tokenizer`\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`~tokenizers.Tokenizer`: The new tokenizer\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |  \n",
      " |  decoder\n",
      " |      The `optional` :class:`~tokenizers.decoders.Decoder` in use by the Tokenizer\n",
      " |  \n",
      " |  model\n",
      " |      The :class:`~tokenizers.models.Model` in use by the Tokenizer\n",
      " |  \n",
      " |  normalizer\n",
      " |      The `optional` :class:`~tokenizers.normalizers.Normalizer` in use by the Tokenizer\n",
      " |  \n",
      " |  padding\n",
      " |      Get the current padding parameters\n",
      " |      \n",
      " |      `Cannot be set, use` :meth:`~tokenizers.Tokenizer.enable_padding` `instead`\n",
      " |      \n",
      " |      Returns:\n",
      " |          (:obj:`dict`, `optional`):\n",
      " |              A dict with the current padding parameters if padding is enabled\n",
      " |  \n",
      " |  post_processor\n",
      " |      The `optional` :class:`~tokenizers.processors.PostProcessor` in use by the Tokenizer\n",
      " |  \n",
      " |  pre_tokenizer\n",
      " |      The `optional` :class:`~tokenizers.pre_tokenizers.PreTokenizer` in use by the Tokenizer\n",
      " |  \n",
      " |  truncation\n",
      " |      Get the currently set truncation parameters\n",
      " |      \n",
      " |      `Cannot set, use` :meth:`~tokenizers.Tokenizer.enable_truncation` `instead`\n",
      " |      \n",
      " |      Returns:\n",
      " |          (:obj:`dict`, `optional`):\n",
      " |              A dict with the current truncation parameters if truncation is enabled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is ', '##this ', '##a ', '##dagg', '##er ', '##which ', '##i see ', '##before ', '##me', '##, the ', '##hand', '##le ', '##toward ', '##my ', '##hand', '##?']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(vocab_size=5000,\n",
    "  special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "tokenizer.train_from_iterator(shakespeare, trainer=trainer)\n",
    "output = tokenizer.encode(sen)\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this a dagger which i see before me, the handle toward my hand?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k',\n",
       " '##ra',\n",
       " '##l',\n",
       " '##sin',\n",
       " '## ',\n",
       " '##as',\n",
       " '##lan',\n",
       " '##sin',\n",
       " '## ',\n",
       " '##macbeth',\n",
       " '##!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for an example of unknown tokens, for some reason this isn't wordpiece\n",
    "\n",
    "tokenizer.encode(\"Kralsin aslansin Macbeth!\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pre-made, but untrained tokeinzers\n",
    "\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "    CharBPETokenizer,\n",
    "    SentencePieceBPETokenizer,\n",
    "    BertWordPieceTokenizer)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
